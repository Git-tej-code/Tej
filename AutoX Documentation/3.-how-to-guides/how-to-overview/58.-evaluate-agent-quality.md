# 58. Evaluate Agent Quality

**Evaluation Methods Available**

1. LLM-as-a-Judge
2. Human Annotation
3. Dataset-based Experiments

&#x20;

**Steps to setup evaluator:**

1. **Setting the Judge Model**

To function correctly, the Agent Observer requires a specific LLM connection to act as the "Judge."

a. Model Requirement: The chosen model must support Structured Output to ensure the system can parse the score and reasoning correctly.

b. Flexibility: The default model can be swapped at any time. Existing historical evaluations remain preserved, while new evaluations utilize the updated model.



2. **Selecting Evaluators**

The Observer can be configured with two types of valuators:

1. **Standard Evaluators**

Pre-built evaluators maintained by the platform. These are "ready to use" and cover common quality dimensions:

* Context-Relevance
* Toxicity
* Helpfulness



2. **Custom Evaluators**

For specific domain requirements, custom evaluators can be drafted:

* Prompt Engineering: Define evaluation prompts using variables like \{{input\}}, \{{output\}}, and \{{ground\_truth\}}.
* Rubric Definition: Customize scoring scales (e.g., binary 0-1 or multi-point) and reasoning guidance.

&#x20;

**What Happens Next**

* Scores are generated
* Results appear in dashboards and also in the LLM as judge page.
* Trends can be tracked over time

&#x20;

**Important Notes**

* Evaluations generate their own traces
* Scores are version-aware
* Evaluations do not modify agents

&#x20;

ðŸ“¸ Screenshot Required

* Evaluation setup screen
* Score results view
